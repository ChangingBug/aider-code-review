"""
æ ¸å¿ƒä»£ç å®¡æŸ¥æœåŠ¡
"""
import os
import re
import json
import shutil
import subprocess
import time
import uuid
from datetime import datetime
from typing import Optional

from git import Repo

from config import config
from database import get_db_session
from models import ReviewRecord, ReviewStatus, ReviewStrategy
from settings import SettingsManager
from utils import (
    logger,
    parse_aider_output,
    filter_valid_files,
    format_review_comment,
    get_commit_prompt,
    get_mr_prompt,
    convert_to_http_auth_url,
    estimate_file_tokens,
    split_files_by_tokens,
    merge_batch_reports
)
from services.git_comment import post_comment_to_git


def run_aider_review(repo_url: str, branch: str, strategy: str, context: dict):
    """
    æ ¸å¿ƒæ‰§è¡Œé€»è¾‘ï¼š
    1. åˆ›å»ºå®¡æŸ¥è®°å½•
    2. å…‹éš†ä»£ç åˆ°æ²™ç›’
    3. è¿è¡ŒAiderè¿›è¡Œå®¡æŸ¥
    4. è§£æè¾“å‡ºå¹¶ä¿å­˜ç»“æœ
    5. å›å†™è¯„è®ºåˆ°Gitå¹³å°
    """
    task_id = str(uuid.uuid4())
    work_dir = os.path.join(config.server.work_dir_base, task_id)
    start_time = datetime.utcnow()
    
    logger.info(f"å¼€å§‹å®¡æŸ¥ä»»åŠ¡ {task_id}, ç­–ç•¥: {strategy}")
    
    # åˆ›å»ºå®¡æŸ¥è®°å½•
    with get_db_session() as db:
        record = ReviewRecord(
            task_id=task_id,
            strategy=ReviewStrategy.COMMIT if strategy == "commit" else ReviewStrategy.MERGE_REQUEST,
            status=ReviewStatus.PROCESSING,
            platform=context.get('platform', 'gitlab'),
            project_id=context.get('project_id'),
            project_name=context.get('project_name'),
            commit_id=context.get('commit_id'),
            mr_iid=context.get('mr_iid'),
            branch=branch,
            target_branch=context.get('target_branch'),
            author_name=context.get('author_name'),
            author_email=context.get('author_email'),
            started_at=start_time,
        )
        db.add(record)
        db.commit()
    
    try:
        # 1. å…‹éš†ä»£ç åˆ°æ²™ç›’
        if os.path.exists(work_dir):
            shutil.rmtree(work_dir)
        
        # ä¼˜å…ˆä½¿ç”¨ä»“åº“çº§è®¤è¯ä¿¡æ¯
        settings = SettingsManager.get_all()
        git_http_user = context.get('repo_http_user') or settings.get('git_http_user', '')
        git_http_password = context.get('repo_http_password') or settings.get('git_http_password', '')
        git_token = context.get('repo_token') or settings.get('git_token', '')
        git_server_url = settings.get('git_server_url', '')
        
        # è½¬æ¢ä¸ºHTTPè®¤è¯URLï¼ˆæ”¯æŒç”¨æˆ·åå¯†ç æˆ–Tokenï¼‰
        clone_url = repo_url
        if (git_http_user and git_http_password) or git_token:
            clone_url = convert_to_http_auth_url(
                repo_url,
                http_user=git_http_user,
                http_password=git_http_password,
                server_url=git_server_url,
                token=git_token
            )
            logger.info(f"ä½¿ç”¨Gitè®¤è¯ä¿¡æ¯å…‹éš†ä»“åº“")
        
        logger.info(f"å…‹éš†ä»“åº“: {repo_url} -> {work_dir}")
        repo = Repo.clone_from(clone_url, work_dir)
        repo.git.checkout(branch)
        
        # 2. æ ¹æ®ç­–ç•¥è·å–å˜æ›´æ–‡ä»¶å’Œæ„å»ºPrompt
        target_files = []
        prompt = ""
        
        if strategy == "commit":
            commit_id = context['commit_id']
            
            # æ£€æŸ¥ç”Ÿæ•ˆæ—¶é—´ - è·³è¿‡åœ¨ effective_time ä¹‹å‰çš„æäº¤
            effective_time_str = context.get('effective_time', '')
            if effective_time_str:
                try:
                    from datetime import datetime
                    # è·å– commit æ—¶é—´
                    commit_time_str = repo.git.log('-1', '--format=%ci', commit_id)
                    commit_time = datetime.fromisoformat(commit_time_str.strip().replace(' ', 'T').replace(' +', '+'))
                    effective_time = datetime.fromisoformat(effective_time_str.replace('Z', '+00:00'))
                    
                    if commit_time < effective_time:
                        logger.info(f"Commit {commit_id[:8]} æ—¶é—´ {commit_time} æ—©äºç”Ÿæ•ˆæ—¶é—´ {effective_time}ï¼Œè·³è¿‡å®¡æŸ¥")
                        finalize_review(task_id, start_time, f"â„¹ï¸ Commit åœ¨ç”Ÿæ•ˆæ—¶é—´ä¹‹å‰ï¼Œå·²è·³è¿‡å®¡æŸ¥ã€‚", 0, 0, 0, 0)
                        return
                except Exception as e:
                    logger.warning(f"è§£æç”Ÿæ•ˆæ—¶é—´å¤±è´¥ï¼Œç»§ç»­å®¡æŸ¥: {e}")
            
            diff_files = repo.git.diff_tree(
                '--no-commit-id', '--name-only', '-r', commit_id
            ).splitlines()
            target_files = diff_files
            prompt = get_commit_prompt()
            logger.info(f"Commit {commit_id[:8]} å˜æ›´äº† {len(diff_files)} ä¸ªæ–‡ä»¶")
            
        elif strategy == "merge_request":
            target_branch = context['target_branch']
            source_ref = context.get('source_ref', '')
            
            # Fetch å¹¶ checkout åˆ° MR æºåˆ†æ”¯
            if source_ref:
                try:
                    # fetch MR çš„æºåˆ†æ”¯ ref
                    logger.info(f"Fetching MR source: {source_ref}")
                    repo.git.fetch('origin', f'{source_ref}:mr_branch')
                    repo.git.checkout('mr_branch')
                    logger.info(f"Checked out to MR source branch")
                except Exception as e:
                    logger.warning(f"Fetch MR source ref å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å½“å‰åˆ†æ”¯: {e}")
            
            # æ£€æŸ¥ç”Ÿæ•ˆæ—¶é—´ - è·³è¿‡åˆ†æ”¯æœ€æ–°æäº¤æ—©äº effective_time çš„ MR
            effective_time_str = context.get('effective_time', '')
            if effective_time_str:
                try:
                    from datetime import datetime
                    # è·å–å½“å‰åˆ†æ”¯æœ€æ–° commit æ—¶é—´
                    commit_time_str = repo.git.log('-1', '--format=%ci')
                    commit_time = datetime.fromisoformat(commit_time_str.strip().replace(' ', 'T').replace(' +', '+'))
                    effective_time = datetime.fromisoformat(effective_time_str.replace('Z', '+00:00'))
                    
                    if commit_time < effective_time:
                        logger.info(f"MR æœ€æ–°æäº¤æ—¶é—´ {commit_time} æ—©äºç”Ÿæ•ˆæ—¶é—´ {effective_time}ï¼Œè·³è¿‡å®¡æŸ¥")
                        finalize_review(task_id, start_time, f"â„¹ï¸ MR æœ€æ–°æäº¤åœ¨ç”Ÿæ•ˆæ—¶é—´ä¹‹å‰ï¼Œå·²è·³è¿‡å®¡æŸ¥ã€‚", 0, 0, 0, 0)
                        return
                except Exception as e:
                    logger.warning(f"è§£æç”Ÿæ•ˆæ—¶é—´å¤±è´¥ï¼Œç»§ç»­å®¡æŸ¥: {e}")
            
            # è·å–ç›¸å¯¹äºç›®æ ‡åˆ†æ”¯çš„å˜æ›´æ–‡ä»¶
            diff_files = repo.git.diff(
                '--name-only', f"origin/{target_branch}"
            ).splitlines()
            target_files = diff_files
            prompt = get_mr_prompt(target_branch)
            logger.info(f"MRç›¸å¯¹äº {target_branch} å˜æ›´äº† {len(diff_files)} ä¸ªæ–‡ä»¶")


        
        # 3. è¿‡æ»¤æœ‰æ•ˆä»£ç æ–‡ä»¶
        valid_files = filter_valid_files(target_files, config.aider.valid_extensions)
        
        # æ›´æ–°æ–‡ä»¶æ•°åˆ°æ•°æ®åº“
        with get_db_session() as db:
            record = db.query(ReviewRecord).filter(ReviewRecord.task_id == task_id).first()
            if record:
                record.files_count = len(valid_files)
                record.files_reviewed = json.dumps(valid_files)
        
        if not valid_files:
            logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„ä»£ç æ–‡ä»¶éœ€è¦å®¡æŸ¥")
            finalize_review(task_id, start_time, "â„¹ï¸ æœ¬æ¬¡å˜æ›´æœªåŒ…å«éœ€è¦å®¡æŸ¥çš„ä»£ç æ–‡ä»¶ã€‚", 0, 0, 0, 0)
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨è¯„è®º
            if SettingsManager.get_bool('enable_comment', True):
                post_comment_to_git(context, "â„¹ï¸ æœ¬æ¬¡å˜æ›´æœªåŒ…å«éœ€è¦å®¡æŸ¥çš„ä»£ç æ–‡ä»¶ã€‚")
            return
        
        logger.info(f"å°†å®¡æŸ¥ {len(valid_files)} ä¸ªä»£ç æ–‡ä»¶: {valid_files}")
        
        # 4. è·å–é…ç½®
        vllm_api_base = settings.get('vllm_api_base', config.vllm.api_base)
        vllm_api_key = settings.get('vllm_api_key', config.vllm.api_key)
        vllm_model_name = settings.get('vllm_model_name', config.vllm.model_name)
        aider_map_tokens = SettingsManager.get_int('aider_map_tokens', config.aider.map_tokens)
        aider_no_repo_map = SettingsManager.get_bool('aider_no_repo_map', config.aider.no_repo_map)
        aider_timeout = SettingsManager.get_int('aider_timeout', 600)
        retry_count = SettingsManager.get_int('aider_retry_count', 1)
        
        # åˆ†æ‰¹é…ç½®ï¼ˆæ–°å¢ï¼‰
        aider_review_max_tokens = SettingsManager.get_int('aider_review_max_tokens', 100000)
        
        env = os.environ.copy()
        env["OPENAI_API_BASE"] = vllm_api_base
        env["OPENAI_API_KEY"] = vllm_api_key
        env["AIDER_MODEL"] = vllm_model_name
        
        # 5. è®¡ç®—æ˜¯å¦éœ€è¦åˆ†æ‰¹
        total_tokens = sum(estimate_file_tokens(os.path.join(work_dir, f)) for f in valid_files)
        
        if total_tokens > aider_review_max_tokens:
            logger.info(f"æ€» token æ•° {total_tokens} è¶…å‡ºé™åˆ¶ {aider_review_max_tokens}ï¼Œå¯ç”¨åˆ†æ‰¹å®¡æŸ¥")
            batches = split_files_by_tokens(valid_files, work_dir, aider_review_max_tokens)
            logger.info(f"æ–‡ä»¶å·²åˆ†ä¸º {len(batches)} æ‰¹")
        else:
            batches = [valid_files]
            logger.info(f"æ€» token æ•° {total_tokens}ï¼Œæ— éœ€åˆ†æ‰¹")
        
        # æ›´æ–°æ‰¹æ¬¡æ€»æ•°åˆ°æ•°æ®åº“
        with get_db_session() as db:
            record = db.query(ReviewRecord).filter(ReviewRecord.task_id == task_id).first()
            if record:
                record.batch_total = len(batches)
                record.batch_current = 0
        
        # 6. å¤šæ‰¹æ¬¡æ‰§è¡Œ Aiderï¼ˆä¿ç•™ Repo Map å…¨ä»“åº“æ„ŸçŸ¥ï¼‰
        batch_reports = []
        batch_results_summary = []  # ç”¨äºå­˜å‚¨æ¯æ‰¹æ¬¡æ‘˜è¦
        
        for batch_idx, batch_files in enumerate(batches):
            logger.info(f"æ‰§è¡Œæ‰¹æ¬¡ {batch_idx + 1}/{len(batches)}: {len(batch_files)} ä¸ªæ–‡ä»¶")
            
            # æ›´æ–°å½“å‰æ‰¹æ¬¡è¿›åº¦
            with get_db_session() as db:
                record = db.query(ReviewRecord).filter(ReviewRecord.task_id == task_id).first()
                if record:
                    record.batch_current = batch_idx + 1

            
            # æ„é€  Aider å‘½ä»¤
            cmd = [
                "aider",
                "--no-auto-commits",
                "--no-git",
                "--yes",
                "--no-pretty",
                "--message", prompt,
            ]
            
            if aider_no_repo_map:
                cmd.append("--no-repo-map")
            else:
                cmd.extend(["--map-tokens", str(aider_map_tokens)])  # ä¿ç•™ Repo Map
            
            cmd.extend(batch_files)
            
            logger.info(f"ä½¿ç”¨æ¨¡å‹: {vllm_model_name}, API: {vllm_api_base}")
            
            # æ‰§è¡Œå¹¶é‡è¯•
            result = None
            last_error = None
            batch_success = True
            
            for attempt in range(retry_count + 1):
                try:
                    result = subprocess.run(
                        cmd,
                        cwd=work_dir,
                        env=env,
                        capture_output=True,
                        text=True,
                        timeout=aider_timeout
                    )
                    
                    if result.returncode == 0:
                        break
                    else:
                        last_error = result.stderr
                        # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯ç”¨äºè¯Šæ–­
                        logger.warning(f"æ‰¹æ¬¡ {batch_idx + 1} å¤±è´¥ (å°è¯• {attempt + 1}/{retry_count + 1})")
                        logger.warning(f"returncode: {result.returncode}")
                        logger.warning(f"stderr: {result.stderr[:500] if result.stderr else '(ç©º)'}")
                        if attempt < retry_count:
                            logger.info(f"ç­‰å¾… 2 ç§’åé‡è¯•...")
                            time.sleep(2)

                        
                except subprocess.TimeoutExpired:
                    last_error = f"æ‰§è¡Œè¶…æ—¶ ({aider_timeout}ç§’)"
                    if attempt < retry_count:
                        logger.warning(f"æ‰¹æ¬¡ {batch_idx + 1} è¶…æ—¶ (å°è¯• {attempt + 1}/{retry_count + 1}), é‡è¯•...")
                    else:
                        # è¶…æ—¶ç”¨å°½é‡è¯•åï¼Œè®°å½•é”™è¯¯ä½†ç»§ç»­åç»­æ‰¹æ¬¡
                        logger.error(f"æ‰¹æ¬¡ {batch_idx + 1} è¶…æ—¶å¤±è´¥ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡ç»§ç»­æ‰§è¡Œ")
                        batch_success = False
            
            # è§£ææ‰¹æ¬¡è¾“å‡º
            if batch_success and result:
                raw_output = result.stdout + result.stderr
                batch_report = parse_aider_output(raw_output)
                batch_status = 'success'
            else:
                batch_report = f"âš ï¸ æ‰¹æ¬¡ {batch_idx + 1} æ‰§è¡Œå¤±è´¥: {last_error}"
                batch_status = 'failed'
            
            batch_reports.append((batch_files, batch_report))
            
            # è®°å½•æ‰¹æ¬¡æ‘˜è¦
            batch_results_summary.append({
                'batch': batch_idx + 1,
                'files': batch_files[:3],  # åªè®°å½•å‰3ä¸ªæ–‡ä»¶å
                'files_count': len(batch_files),
                'status': batch_status,
                'preview': batch_report[:200] if batch_report else ''  # é¢„è§ˆå‰200å­—ç¬¦
            })
            
            # æ›´æ–°æ‰¹æ¬¡ç»“æœåˆ°æ•°æ®åº“
            with get_db_session() as db:
                record = db.query(ReviewRecord).filter(ReviewRecord.task_id == task_id).first()
                if record:
                    record.batch_results = json.dumps(batch_results_summary, ensure_ascii=False)
            
            if result and result.returncode != 0:
                logger.warning(f"æ‰¹æ¬¡ {batch_idx + 1} è¿”å›éé›¶çŠ¶æ€: {last_error}")

        
        # 7. åˆå¹¶æŠ¥å‘Š
        if len(batch_reports) > 1:
            review_report = merge_batch_reports(batch_reports)
            logger.info(f"å·²åˆå¹¶ {len(batch_reports)} ä¸ªæ‰¹æ¬¡çš„æŠ¥å‘Š")
        else:
            review_report = batch_reports[0][1] if batch_reports else "âš ï¸ æœªè·å–åˆ°å®¡æŸ¥ç»“æœ"

        
        # 8. åˆ†æé—®é¢˜æ•°é‡
        critical, warning, suggestion = analyze_issues(review_report)
        total_issues = critical + warning + suggestion
        
        # è®¡ç®—è´¨é‡è¯„åˆ† (ç®€å•ç®—æ³•: 100 - é—®é¢˜åŠ æƒ)
        quality_score = max(0, 100 - (critical * 20 + warning * 5 + suggestion * 1))
        
        # 9. ä¿å­˜ç»“æœ
        formatted_report = format_review_comment(review_report, strategy, context)
        finalize_review(task_id, start_time, formatted_report, total_issues, critical, warning, suggestion, quality_score)
        
        # 10. å›å†™è¯„è®ºï¼ˆä¼˜å…ˆä½¿ç”¨ä»“åº“çº§å¼€å…³ï¼Œfallbackåˆ°å…¨å±€é…ç½®ï¼‰
        enable_comment = context.get('enable_comment', SettingsManager.get_bool('enable_comment', True))
        if enable_comment:
            post_comment_to_git(context, formatted_report)
        else:
            logger.info("è¯„è®ºå›å†™å·²ç¦ç”¨ï¼Œè·³è¿‡")
        
        logger.info(f"ä»»åŠ¡ {task_id} å®Œæˆ, å‘ç° {total_issues} ä¸ªé—®é¢˜")
        
    except subprocess.TimeoutExpired:
        logger.error(f"ä»»åŠ¡ {task_id} è¶…æ—¶ (å·²ç”¨å°½æ‰€æœ‰é‡è¯•)")
        finalize_review(task_id, start_time, None, 0, 0, 0, 0, error="ä»»åŠ¡è¶…æ—¶")
        enable_comment = context.get('enable_comment', SettingsManager.get_bool('enable_comment', True))
        if enable_comment:
            post_comment_to_git(context, "âš ï¸ ä»£ç å®¡æŸ¥è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•æˆ–å‡å°‘å˜æ›´æ–‡ä»¶æ•°é‡ã€‚")
    except Exception as e:
        logger.exception(f"ä»»åŠ¡ {task_id} æ‰§è¡Œå¤±è´¥: {e}")
        finalize_review(task_id, start_time, None, 0, 0, 0, 0, error=str(e))
        enable_comment = context.get('enable_comment', SettingsManager.get_bool('enable_comment', True))
        if enable_comment:
            post_comment_to_git(context, f"âŒ ä»£ç å®¡æŸ¥æ‰§è¡Œå¤±è´¥: {str(e)}")
    finally:
        if os.path.exists(work_dir):
            try:
                shutil.rmtree(work_dir)
                logger.info(f"æ¸…ç†å·¥ä½œç›®å½•: {work_dir}")
            except Exception as e:
                logger.warning(f"æ¸…ç†å·¥ä½œç›®å½•å¤±è´¥: {e}")


def finalize_review(task_id: str, start_time: datetime, report: Optional[str], 
                    issues: int, critical: int, warning: int, suggestion: int,
                    quality_score: float = None, error: str = None):
    """å®Œæˆå®¡æŸ¥è®°å½•çš„æ›´æ–°"""
    end_time = datetime.utcnow()
    processing_time = (end_time - start_time).total_seconds()
    
    with get_db_session() as db:
        record = db.query(ReviewRecord).filter(ReviewRecord.task_id == task_id).first()
        if record:
            record.status = ReviewStatus.FAILED if error else ReviewStatus.COMPLETED
            record.completed_at = end_time
            record.processing_time_seconds = processing_time
            record.report = report
            record.issues_count = issues
            record.critical_count = critical
            record.warning_count = warning
            record.suggestion_count = suggestion
            record.quality_score = quality_score
            record.error_message = error


def analyze_issues(report: str) -> tuple:
    """åˆ†æå®¡æŸ¥æŠ¥å‘Šä¸­çš„é—®é¢˜æ•°é‡"""
    if not report:
        return 0, 0, 0
    
    # ç®€å•çš„é—®é¢˜è¯†åˆ«é€»è¾‘ï¼ŒåŸºäºå…³é”®è¯
    critical = len(re.findall(r'ğŸ”´|ä¸¥é‡|critical|error|security|æ¼æ´|å±é™©', report, re.IGNORECASE))
    warning = len(re.findall(r'ğŸŸ¡|è­¦å‘Š|warning|æ³¨æ„|é—®é¢˜', report, re.IGNORECASE))
    suggestion = len(re.findall(r'ğŸ”µ|å»ºè®®|suggestion|ä¼˜åŒ–|æ”¹è¿›|recommend', report, re.IGNORECASE))
    
    return critical, warning, suggestion
